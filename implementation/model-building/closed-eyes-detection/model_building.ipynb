{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.config.list_physical_devices('GPU')\n",
    "\n",
    "from keras import applications, Input, Model, optimizers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten\n",
    "from keras.callbacks import EarlyStopping, TensorBoard"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRL eye data set path:\n",
      "t:\\498-dl\\modules\\project\\implementation\\model-training\\closed-eyes-detection\\data\\MRL_eye_dataset\\eyes\n",
      "Own eye data set path:\n",
      "t:\\498-dl\\modules\\project\\implementation\\model-training\\closed-eyes-detection\\data\\own_eye_dataset\\eyes\n",
      "\n",
      "Tensorboard log path:\n",
      "t:\\498-dl\\modules\\project\\implementation\\model-training\\closed-eyes-detection\\log_dir\n",
      "\n",
      "Custom model path:\n",
      "t:\\498-dl\\modules\\project\\implementation\\model-training\\closed-eyes-detection\\custom\\open_closed_eyes-001\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "target_size = 96\n",
    "batch_size = 512\n",
    "epochs = 100\n",
    "early_stopping_patience = 10\n",
    "\n",
    "mrl_dataset_path = os.path.join(os.getcwd(), 'data\\\\MRL_eye_dataset\\\\eyes')\n",
    "print(f'MRL eye data set path:\\n{mrl_dataset_path}')\n",
    "\n",
    "own_dataset_path = os.path.join(os.getcwd(), 'data\\\\own_eye_dataset\\\\eyes')\n",
    "print(f'Own eye data set path:\\n{own_dataset_path}')\n",
    "\n",
    "tensorboard_log_dir = os.path.join(os.getcwd(), 'log_dir')\n",
    "print(f'\\nTensorboard log path:\\n{tensorboard_log_dir}')\n",
    "\n",
    "custom_model_path = os.path.join(os.getcwd(), 'custom\\\\open_closed_eyes-001')\n",
    "print(f'\\nCustom model path:\\n{custom_model_path}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. MRL Eye Data Set"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3172 images belonging to 2 classes.\n",
      "Found 954 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# augmented images\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        zoom_range=0.1,\n",
    "        horizontal_flip=True,\n",
    "        rotation_range=5,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        # represents 80 percent of the data set\n",
    "        os.path.join(mrl_dataset_path, 'train'),\n",
    "        target_size=(target_size, target_size),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary',\n",
    "        seed=seed)\n",
    "\n",
    "# unchanged images\n",
    "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "validation_generator = valid_datagen.flow_from_directory(\n",
    "        # represents 20 percent of the data set\n",
    "        os.path.join(mrl_dataset_path, 'validate'),\n",
    "        target_size=(target_size, target_size),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using feature extractors from MobileNet\n",
    "base_model =  applications.MobileNet(include_top = True , weights = 'imagenet', input_tensor = Input(shape = (target_size, target_size, 3)))\n",
    "for models in base_model.layers:\n",
    "  models.trainable= False\n",
    "base_model = Model(inputs = base_model.input, outputs = base_model.layers[-2].output)\n",
    "\n",
    "# Build model using MobileNet feature extractors, 2 fully connected layers, and a softmax output\n",
    "model = Sequential()\n",
    "for layer in base_model.layers:\n",
    "  model.add(layer)\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# compile model\n",
    "model.compile(optimizer=optimizers.Adam(0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.4692 - accuracy: 0.4805 - val_loss: 0.8100 - val_accuracy: 0.6309\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.8587 - accuracy: 0.5781 - val_loss: 0.4089 - val_accuracy: 0.8066\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 1s 934ms/step - loss: 0.5182 - accuracy: 0.7500 - val_loss: 0.3185 - val_accuracy: 0.8516\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.4112 - accuracy: 0.8320 - val_loss: 0.3440 - val_accuracy: 0.8652\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.3655 - accuracy: 0.8301 - val_loss: 0.3666 - val_accuracy: 0.8613\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.3180 - accuracy: 0.8730 - val_loss: 0.3303 - val_accuracy: 0.8926\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.3345 - accuracy: 0.8594 - val_loss: 0.2749 - val_accuracy: 0.9082\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.3231 - accuracy: 0.8809 - val_loss: 0.2327 - val_accuracy: 0.9121\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 1s 865ms/step - loss: 0.2030 - accuracy: 0.9400 - val_loss: 0.2103 - val_accuracy: 0.9219\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.2678 - accuracy: 0.9004 - val_loss: 0.2125 - val_accuracy: 0.9238\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.2793 - accuracy: 0.8887 - val_loss: 0.2062 - val_accuracy: 0.9180\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.2884 - accuracy: 0.8809 - val_loss: 0.1863 - val_accuracy: 0.9297\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.2415 - accuracy: 0.8965 - val_loss: 0.2609 - val_accuracy: 0.9102\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 1s 871ms/step - loss: 0.1658 - accuracy: 0.9600 - val_loss: 0.3033 - val_accuracy: 0.9160\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.2002 - accuracy: 0.9102 - val_loss: 0.3276 - val_accuracy: 0.8828\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.2364 - accuracy: 0.9043 - val_loss: 0.2330 - val_accuracy: 0.9199\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.1821 - accuracy: 0.9316 - val_loss: 0.2778 - val_accuracy: 0.9180\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.2326 - accuracy: 0.9180 - val_loss: 0.2175 - val_accuracy: 0.9297\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.2266 - accuracy: 0.9180 - val_loss: 0.1768 - val_accuracy: 0.9355\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.2054 - accuracy: 0.9238 - val_loss: 0.1679 - val_accuracy: 0.9316\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.2088 - accuracy: 0.9121 - val_loss: 0.2071 - val_accuracy: 0.9258\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.2184 - accuracy: 0.9082 - val_loss: 0.1578 - val_accuracy: 0.9453\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.1760 - accuracy: 0.9219 - val_loss: 0.1583 - val_accuracy: 0.9395\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.2195 - accuracy: 0.9102 - val_loss: 0.2322 - val_accuracy: 0.9238\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.1603 - accuracy: 0.9355 - val_loss: 0.2072 - val_accuracy: 0.9277\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.1818 - accuracy: 0.9238 - val_loss: 0.2160 - val_accuracy: 0.9297\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.1857 - accuracy: 0.9336 - val_loss: 0.1930 - val_accuracy: 0.9336\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.2146 - accuracy: 0.9277 - val_loss: 0.1903 - val_accuracy: 0.9395\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.1669 - accuracy: 0.9336 - val_loss: 0.1634 - val_accuracy: 0.9395\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.1682 - accuracy: 0.9375 - val_loss: 0.1631 - val_accuracy: 0.9434\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.1754 - accuracy: 0.9258 - val_loss: 0.1677 - val_accuracy: 0.9395\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2200 - accuracy: 0.9219Restoring model weights from the end of the best epoch: 22.\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.2200 - accuracy: 0.9219 - val_loss: 0.1856 - val_accuracy: 0.9336\n",
      "Epoch 32: early stopping\n"
     ]
    }
   ],
   "source": [
    "early_stopping_callback = EarlyStopping(monitor = 'val_loss', patience = early_stopping_patience, restore_best_weights = True, verbose = 1)\n",
    "tensorboard_callback = TensorBoard(log_dir = tensorboard_log_dir, histogram_freq = 1)\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=1000//batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data = validation_generator,\n",
    "    validation_steps=800//batch_size,\n",
    "    callbacks=[early_stopping_callback, tensorboard_callback])\n",
    "\n",
    "# %tensorboard --logdir log_dir"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation using unseen test set representing 15% of the MRL eye data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 720 images belonging to 2 classes.\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.1554 - accuracy: 0.9292\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.15540042519569397, 0.9291666746139526]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mrl_test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "mrl_test_generator = mrl_test_datagen.flow_from_directory(\n",
    "        os.path.join(mrl_dataset_path, 'test'),\n",
    "        target_size=(target_size, target_size),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')\n",
    "\n",
    "model.evaluate(mrl_test_generator)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation using the targeted data (recorded video from the system) using the unseen test set representing 15% of the own eye data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 470 images belonging to 2 classes.\n",
      "1/1 [==============================] - 0s 177ms/step - loss: 1.8924 - accuracy: 0.4830\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.8924071788787842, 0.4829787313938141]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "own_test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "own_test_generator = own_test_datagen.flow_from_directory(\n",
    "        os.path.join(own_dataset_path, 'test'),\n",
    "        target_size=(target_size, target_size),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')\n",
    "\n",
    "model.evaluate(own_test_generator)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### >>> Result: Data sets are incompatible (Low-Light IR vs. (A)BGR capture)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This most likely explains the observed weak performance of the pretrained OpenVINO model on the recorded data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Own Eye Data Set"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Model 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2092 images belonging to 2 classes.\n",
      "Found 630 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# augmented images\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        zoom_range=0.1,\n",
    "        horizontal_flip=True,\n",
    "        rotation_range=5,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        # represents 80 percent of the data set\n",
    "        os.path.join(own_dataset_path, 'train'),\n",
    "        target_size=(target_size, target_size),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')\n",
    "\n",
    "# unchanged images\n",
    "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "validation_generator = valid_datagen.flow_from_directory(\n",
    "        # represents 20 percent of the data set\n",
    "        os.path.join(own_dataset_path, 'validate'),\n",
    "        target_size=(target_size, target_size),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Building"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model will use the feature extractors from MobileNet and add two fully connected dense layers on top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using feature extractors from MobileNet\n",
    "base_model =  applications.MobileNet(include_top = True , weights = 'imagenet', input_tensor = Input(shape = (target_size, target_size, 3)))\n",
    "for models in base_model.layers:\n",
    "  models.trainable= False\n",
    "base_model = Model(inputs = base_model.input, outputs = base_model.layers[-2].output)\n",
    "\n",
    "# Build model using MobileNet feature extractors, 2 fully connected layers, and a softmax output\n",
    "model = Sequential()\n",
    "for layer in base_model.layers:\n",
    "  model.add(layer)\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# compile model\n",
    "model.compile(optimizer=optimizers.Adam(0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.6868 - accuracy: 0.4091 - val_loss: 0.6022 - val_accuracy: 0.7207\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.5505 - accuracy: 0.7480 - val_loss: 0.5748 - val_accuracy: 0.7637\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 1s 803ms/step - loss: 0.8000 - accuracy: 0.6591 - val_loss: 0.4364 - val_accuracy: 0.7637\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.3954 - accuracy: 0.7461 - val_loss: 0.2547 - val_accuracy: 0.8711\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.2169 - accuracy: 0.8887 - val_loss: 0.1790 - val_accuracy: 0.9355\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.1554 - accuracy: 0.9570 - val_loss: 0.1577 - val_accuracy: 0.9336\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.1535 - accuracy: 0.9297 - val_loss: 0.1482 - val_accuracy: 0.9375\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 1s 806ms/step - loss: 0.1298 - accuracy: 0.9545 - val_loss: 0.1516 - val_accuracy: 0.9316\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.1736 - accuracy: 0.9102 - val_loss: 0.1313 - val_accuracy: 0.9434\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.1775 - accuracy: 0.9141 - val_loss: 0.1032 - val_accuracy: 0.9492\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.1194 - accuracy: 0.9551 - val_loss: 0.0804 - val_accuracy: 0.9609\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 1s 849ms/step - loss: 0.0840 - accuracy: 0.9773 - val_loss: 0.0736 - val_accuracy: 0.9727\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 1s 838ms/step - loss: 0.0278 - accuracy: 1.0000 - val_loss: 0.0708 - val_accuracy: 0.9844\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 1s 912ms/step - loss: 0.1284 - accuracy: 0.9545 - val_loss: 0.0804 - val_accuracy: 0.9668\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.1141 - accuracy: 0.9707 - val_loss: 0.0868 - val_accuracy: 0.9629\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0760 - accuracy: 0.9648 - val_loss: 0.0948 - val_accuracy: 0.9609\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.1240 - accuracy: 0.9531 - val_loss: 0.0773 - val_accuracy: 0.9648\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0739 - accuracy: 0.9707 - val_loss: 0.0686 - val_accuracy: 0.9766\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0635 - accuracy: 0.9805 - val_loss: 0.0449 - val_accuracy: 0.9922\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0666 - accuracy: 0.9785 - val_loss: 0.0477 - val_accuracy: 0.9863\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 1s 808ms/step - loss: 0.1298 - accuracy: 0.9773 - val_loss: 0.0500 - val_accuracy: 0.9785\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0466 - accuracy: 0.9805 - val_loss: 0.0472 - val_accuracy: 0.9785\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0999 - accuracy: 0.9570 - val_loss: 0.0488 - val_accuracy: 0.9746\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.1131 - accuracy: 0.9648 - val_loss: 0.0524 - val_accuracy: 0.9766\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.1033 - accuracy: 0.9707 - val_loss: 0.0383 - val_accuracy: 0.9844\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0603 - accuracy: 0.9746 - val_loss: 0.0376 - val_accuracy: 0.9824\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0703 - accuracy: 0.9844 - val_loss: 0.0324 - val_accuracy: 0.9883\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0680 - accuracy: 0.9727 - val_loss: 0.0363 - val_accuracy: 0.9902\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 1s 772ms/step - loss: 0.1454 - accuracy: 0.9091 - val_loss: 0.0374 - val_accuracy: 0.9863\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0759 - accuracy: 0.9688 - val_loss: 0.0384 - val_accuracy: 0.9883\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0919 - accuracy: 0.9668 - val_loss: 0.0371 - val_accuracy: 0.9902\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0745 - accuracy: 0.9629 - val_loss: 0.0345 - val_accuracy: 0.9922\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0803 - accuracy: 0.9707 - val_loss: 0.0303 - val_accuracy: 0.9941\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 1s 844ms/step - loss: 0.0322 - accuracy: 1.0000 - val_loss: 0.0331 - val_accuracy: 0.9941\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0742 - accuracy: 0.9746 - val_loss: 0.0322 - val_accuracy: 0.9902\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0772 - accuracy: 0.9785 - val_loss: 0.0317 - val_accuracy: 0.9922\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0635 - accuracy: 0.9785 - val_loss: 0.0260 - val_accuracy: 0.9980\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 1s 749ms/step - loss: 0.0339 - accuracy: 0.9773 - val_loss: 0.0300 - val_accuracy: 0.9941\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 1s 736ms/step - loss: 0.0201 - accuracy: 1.0000 - val_loss: 0.0325 - val_accuracy: 0.9922\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0558 - accuracy: 0.9746 - val_loss: 0.0266 - val_accuracy: 0.9922\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0404 - accuracy: 0.9863 - val_loss: 0.0277 - val_accuracy: 0.9922\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 1s 843ms/step - loss: 0.0138 - accuracy: 1.0000 - val_loss: 0.0363 - val_accuracy: 0.9883\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0429 - accuracy: 0.9863 - val_loss: 0.0328 - val_accuracy: 0.9902\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0420 - accuracy: 0.9805 - val_loss: 0.0385 - val_accuracy: 0.9883\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0417 - accuracy: 0.9902 - val_loss: 0.0336 - val_accuracy: 0.9883\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 1s 823ms/step - loss: 0.0436 - accuracy: 0.9545 - val_loss: 0.0337 - val_accuracy: 0.9863\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0775 - accuracy: 0.9727Restoring model weights from the end of the best epoch: 37.\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0775 - accuracy: 0.9727 - val_loss: 0.0369 - val_accuracy: 0.9883\n",
      "Epoch 47: early stopping\n"
     ]
    }
   ],
   "source": [
    "early_stopping_callback = EarlyStopping(monitor = 'val_loss', patience = early_stopping_patience, restore_best_weights = True, verbose = 1)\n",
    "tensorboard_callback = TensorBoard(log_dir = tensorboard_log_dir, histogram_freq = 1)\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=1000//batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data = validation_generator,\n",
    "    validation_steps=800//batch_size,\n",
    "    callbacks=[early_stopping_callback, tensorboard_callback])\n",
    "\n",
    "# enable to add logs for tensorboard\n",
    "# %tensorboard --logdir log_dir"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation using the targeted data (recorded video from the system) using the unseen test set representing 15% of the own eye data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 470 images belonging to 2 classes.\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.0737 - accuracy: 0.9787\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0736980140209198, 0.978723406791687]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "own_test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "own_test_generator = own_test_datagen.flow_from_directory(\n",
    "        os.path.join(own_dataset_path, 'test'),\n",
    "        target_size=(target_size, target_size),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')\n",
    "\n",
    "model.evaluate(own_test_generator)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### >>> Result: Accuracy should be sufficient for target data, if samples are representative.<br><br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export the model to the distracted driver detection project for conversion to openVINO intermediate format (IR)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Custom model path:\n",
      "t:\\498-dl\\modules\\project\\implementation\\distracted-driving-detection\\models\\custom\\open-closed-eyes-001\n"
     ]
    }
   ],
   "source": [
    "custom_model_path = os.path.join(os.path.dirname(os.path.dirname(os.getcwd())), \n",
    "                                 'distracted-driving-detection\\\\models\\\\custom\\\\open-closed-eyes-001')\n",
    "print(f'\\nCustom model path:\\n{custom_model_path}')\n",
    "\n",
    "#\n",
    "# NOTE: uncomment to export model\n",
    "#\n",
    "# model.save(custom_model_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Model 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to reduce complexity and thus inference time a second model is built."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same data is used for training model 2 as for model 1."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Conv2D(32,(3,3),activation='relu',input_shape=(target_size, target_size, 3)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.05))\n",
    "model.add(Conv2D(64,(3,3),activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.05))\n",
    "model.add(Conv2D(128,(3,3),activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.05))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64,activation='relu'))\n",
    "model.add(Dropout(0.05))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "\n",
    "model.compile(optimizer=optimizers.Adam(0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/125\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.7654 - accuracy: 0.1992 - val_loss: 0.6014 - val_accuracy: 0.7617\n",
      "Epoch 2/125\n",
      "1/1 [==============================] - 1s 780ms/step - loss: 0.5736 - accuracy: 0.7578 - val_loss: 0.5360 - val_accuracy: 0.7422\n",
      "Epoch 3/125\n",
      "1/1 [==============================] - 1s 791ms/step - loss: 0.5309 - accuracy: 0.7656 - val_loss: 0.5159 - val_accuracy: 0.7578\n",
      "Epoch 4/125\n",
      "1/1 [==============================] - 1s 786ms/step - loss: 0.5422 - accuracy: 0.7598 - val_loss: 0.5280 - val_accuracy: 0.7734\n",
      "Epoch 5/125\n",
      "1/1 [==============================] - 1s 825ms/step - loss: 0.4926 - accuracy: 0.7930 - val_loss: 0.5440 - val_accuracy: 0.7598\n",
      "Epoch 6/125\n",
      "1/1 [==============================] - 1s 758ms/step - loss: 0.5158 - accuracy: 0.7734 - val_loss: 0.5316 - val_accuracy: 0.7598\n",
      "Epoch 7/125\n",
      "1/1 [==============================] - 1s 797ms/step - loss: 0.5220 - accuracy: 0.7598 - val_loss: 0.5039 - val_accuracy: 0.7754\n",
      "Epoch 8/125\n",
      "1/1 [==============================] - 1s 830ms/step - loss: 0.5209 - accuracy: 0.7656 - val_loss: 0.5301 - val_accuracy: 0.7480\n",
      "Epoch 9/125\n",
      "1/1 [==============================] - 1s 778ms/step - loss: 0.5336 - accuracy: 0.7520 - val_loss: 0.5227 - val_accuracy: 0.7617\n",
      "Epoch 10/125\n",
      "1/1 [==============================] - 0s 492ms/step - loss: 0.4236 - accuracy: 0.8409 - val_loss: 0.5239 - val_accuracy: 0.7520\n",
      "Epoch 11/125\n",
      "1/1 [==============================] - 1s 794ms/step - loss: 0.5196 - accuracy: 0.7656 - val_loss: 0.5062 - val_accuracy: 0.7676\n",
      "Epoch 12/125\n",
      "1/1 [==============================] - 1s 764ms/step - loss: 0.5091 - accuracy: 0.7715 - val_loss: 0.5102 - val_accuracy: 0.7676\n",
      "Epoch 13/125\n",
      "1/1 [==============================] - 1s 819ms/step - loss: 0.4634 - accuracy: 0.8047 - val_loss: 0.5032 - val_accuracy: 0.7793\n",
      "Epoch 14/125\n",
      "1/1 [==============================] - 1s 804ms/step - loss: 0.4817 - accuracy: 0.7930 - val_loss: 0.5011 - val_accuracy: 0.7773\n",
      "Epoch 15/125\n",
      "1/1 [==============================] - 1s 849ms/step - loss: 0.5064 - accuracy: 0.7715 - val_loss: 0.5101 - val_accuracy: 0.7676\n",
      "Epoch 16/125\n",
      "1/1 [==============================] - 1s 818ms/step - loss: 0.5183 - accuracy: 0.7578 - val_loss: 0.5265 - val_accuracy: 0.7539\n",
      "Epoch 17/125\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 0.5726 - accuracy: 0.7273 - val_loss: 0.5383 - val_accuracy: 0.7695\n",
      "Epoch 18/125\n",
      "1/1 [==============================] - 1s 842ms/step - loss: 0.5237 - accuracy: 0.7539 - val_loss: 0.5510 - val_accuracy: 0.7559\n",
      "Epoch 19/125\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 0.4022 - accuracy: 0.8864 - val_loss: 0.5101 - val_accuracy: 0.7656\n",
      "Epoch 20/125\n",
      "1/1 [==============================] - 1s 805ms/step - loss: 0.4920 - accuracy: 0.7754 - val_loss: 0.5009 - val_accuracy: 0.7480\n",
      "Epoch 21/125\n",
      "1/1 [==============================] - 1s 823ms/step - loss: 0.4974 - accuracy: 0.7793 - val_loss: 0.4989 - val_accuracy: 0.7520\n",
      "Epoch 22/125\n",
      "1/1 [==============================] - 1s 791ms/step - loss: 0.5371 - accuracy: 0.7520 - val_loss: 0.4976 - val_accuracy: 0.7637\n",
      "Epoch 23/125\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4675 - accuracy: 0.7891Restoring model weights from the end of the best epoch: 13.\n",
      "1/1 [==============================] - 1s 830ms/step - loss: 0.4675 - accuracy: 0.7891 - val_loss: 0.5129 - val_accuracy: 0.7676\n",
      "Epoch 23: early stopping\n"
     ]
    }
   ],
   "source": [
    "early_stopping_callback = EarlyStopping(monitor = 'val_accuracy', patience = early_stopping_patience, restore_best_weights = True, verbose = 1)\n",
    "tensorboard_callback = TensorBoard(log_dir = tensorboard_log_dir, histogram_freq = 1)\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=1000//batch_size,\n",
    "    epochs=125,\n",
    "    validation_data = validation_generator,\n",
    "    validation_steps=800//batch_size,\n",
    "    callbacks=[early_stopping_callback, tensorboard_callback])\n",
    "\n",
    "# %tensorboard --logdir log_dir"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 470 images belonging to 2 classes.\n",
      "1/1 [==============================] - 1s 505ms/step - loss: 0.5170 - accuracy: 0.7660\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5170083045959473, 0.7659574747085571]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "own_test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "own_test_generator = own_test_datagen.flow_from_directory(\n",
    "        os.path.join(own_dataset_path, 'test'),\n",
    "        target_size=(target_size, target_size),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')\n",
    "\n",
    "model.evaluate(own_test_generator)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Custom model path:\n",
      "t:\\498-dl\\modules\\project\\implementation\\distracted-driving-detection\\models\\custom\\open-closed-eyes-002\n"
     ]
    }
   ],
   "source": [
    "custom_model_path = os.path.join(os.path.dirname(os.path.dirname(os.getcwd())), \n",
    "                                 'distracted-driving-detection\\\\models\\\\custom\\\\open-closed-eyes-002')\n",
    "print(f'\\nCustom model path:\\n{custom_model_path}')\n",
    "\n",
    "#\n",
    "# NOTE: uncomment to export model\n",
    "#\n",
    "# model.save(custom_model_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If none of the models performs well in the pipeline then most likely the data is lacking variability and more data needs to be acquired.<br>Due to project time constraints, however, new/more data can most likely not be acquired for now."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openvino_en",
   "language": "python",
   "name": "openvino_en"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
